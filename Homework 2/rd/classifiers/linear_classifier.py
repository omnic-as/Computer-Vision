from __future__ import print_function

from builtins import object
from rd.classifiers.linear_svm import *


class LinearClassifier(object):

    def __init__(self):
        self.W = None

    def train(self, X, y, learning_rate=1e-3, reg=1e-5, num_iters=100,
              batch_size=200, verbose=False):
        """
        Обучение линейного классификатора с помощью СГС.

        Аргументы:
        - X: массив размера (N, D), содержит партию из N тренировочных данных размера D
        - y: массив тренировочных меток размера (N,)
        - learning_rate: (float) скорость обучения для оптимизации
        - reg: (float) сила/уровень регуляризации
        - num_iters: (integer) количество шагов при оптимизации
        - batch_size: (integer) количество обучающих примеров для использования на каждом шагу
        - verbose: (boolean) Если true, выводить прогресс во время оптимизации.

        Возвращает:
        Список со значением функции потерь на каждой итерации обучения.
        """
        num_train, dim = X.shape
        num_classes = np.max(y) + 1
        if self.W is None:
            self.W = 0.001 * np.random.randn(dim, num_classes)

        # СГС для оптимизации W
        loss_history = []
        for it in range(num_iters):
            X_batch = None
            y_batch = None

            #########################################################################
            # TODO:                                                                 #
            # Выделите batch_size элементов из обучающих данных в X_batch и их      #
            # соответствующие метки в y_batch. После выборки X_batch должен иметь   #
            # форму (batch_size, dim) и y_batch - форму (batch_size,                #
            #                                                                       #
            # Подсказка: Используйте np.random.choice для генерации индексов.       #
            #########################################################################
            # *****НАЧАЛО ВАШЕГО КОДА (НЕ УДАЛЯЙТЕ / ИЗМЕНЯЙТЕ ЭТУ СТРОКУ)*****

            pass

            # *****КОНЕЦ ВАШЕГО КОДА (НЕ УДАЛЯЙТЕ / ИЗМЕНЯЙТЕ ЭТУ СТРОКУ)*****

            loss, grad = self.loss(X_batch, y_batch, reg)
            loss_history.append(loss)

            # обновление параметров
            #########################################################################
            # TODO:                                                                 #
            # Обновите веса, используя градиент и скорость обучения.                #
            #########################################################################
            # *****НАЧАЛО ВАШЕГО КОДА (НЕ УДАЛЯЙТЕ / ИЗМЕНЯЙТЕ ЭТУ СТРОКУ)*****

            pass

            # *****КОНЕЦ ВАШЕГО КОДА (НЕ УДАЛЯЙТЕ / ИЗМЕНЯЙТЕ ЭТУ СТРОКУ)*****

            if verbose and it % 100 == 0:
                print('итерация %d / %d: потеря %f' % (it, num_iters, loss))

        return loss_history

    def predict(self, X):
        """
        Используйте обученные веса линейного классификатора, чтобы предсказать метки.

        Аргументы:
        - X: массив тестовых данных

        Возвращает:
        - y: массив предсказанных меток для данных в Х
        """
        y_pred = np.zeros(X.shape[0])
        ###########################################################################
        # TODO:                                                                   #
        # Реализуйте этот метод. Сохраните предсказанные метки в y_pred.          #
        ###########################################################################
        # *****НАЧАЛО ВАШЕГО КОДА (НЕ УДАЛЯЙТЕ / ИЗМЕНЯЙТЕ ЭТУ СТРОКУ)*****

        pass

        # *****КОНЕЦ ВАШЕГО КОДА (НЕ УДАЛЯЙТЕ / ИЗМЕНЯЙТЕ ЭТУ СТРОКУ)*****
        return y_pred

    def loss(self, X_batch, y_batch, reg):
        """
        Вычисляет функцию потерь и ее производную.
        Подклассы переопределят этот метод.

        Аргументы:
        - X_batch: массив размера (N, D), содержит партию из N тренировочных данных
        - y_batch: массив тренировочных меток размера (N,)
        - reg: (float) уровень регуляризации

        Возвращает:
        - потеря (float)
        - градиент функции по отношению к весам в W; массив такого же размера, как  W
        """
        pass


class LinearSVM(LinearClassifier):

    def loss(self, X_batch, y_batch, reg):
        return svm_loss_vectorized(self.W, X_batch, y_batch, reg)